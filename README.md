# CS5720 - Neural Networks and Deep Learning  
### Home Assignment 1 â€“ Summer 2025  
**Student Name:** shreya surakanti  
**University of Central Missouri**  
**Course:** CS5720 Neural Networks and Deep Learning  
**Instructor:** [Instructor Name, if applicable]

---

## ğŸ“‚ Assignment Overview

This assignment is divided into three key parts:

1. **Tensor Manipulations & Reshaping**
2. **Loss Functions & Hyperparameter Tuning**
3. **Neural Network Training with TensorBoard**

---

## ğŸ“Œ Part 1: Tensor Manipulations & Reshaping

### âœ… Tasks Completed:
- Created a random tensor with shape **(4, 6)** using TensorFlow.
- Found its **rank** and **shape**.
- Reshaped to **(2, 3, 4)** and transposed to **(3, 2, 4)**.
- Broadcasted a tensor of shape **(1, 4)** and added it.
  
### ğŸ“ˆ Outputs:
- **Original Shape:** (4, 6)  
- **Rank:** 2  
- **Reshaped Shape:** (2, 3, 4)  
- **Transposed Shape:** (3, 2, 4)

### ğŸ§  Broadcasting Explained:
TensorFlow broadcasting automatically expands the smaller tensor to match the shape of the larger one along compatible dimensions. This allows element-wise operations without needing explicit reshaping.

---

## ğŸ“Œ Part 2: Loss Functions & Hyperparameter Tuning

### âœ… Tasks Completed:
- Defined sample true labels and model predictions.
- Calculated:
  - **Mean Squared Error (MSE)**
  - **Categorical Cross-Entropy (CCE)**
- Modified predictions and recalculated losses.
- Plotted a bar chart comparing MSE and CCE.

### ğŸ“ˆ Results:
| Prediction Version | MSE Loss | CCE Loss |
|--------------------|----------|----------|
| Initial             | 0.0167   | 0.3562   |
| Modified            | 0.0102   | 0.2594   |

### ğŸ§  Insight:
- MSE is more sensitive to small numeric differences.
- Cross-Entropy is more appropriate for classification tasks as it penalizes incorrect class probabilities more heavily.

---

## ğŸ“Œ Part 3: Neural Network Training with TensorBoard

### âœ… Tasks Completed:
- Loaded and preprocessed the **MNIST** dataset.
- Built a simple neural network model.
- Trained the model for **5 epochs** with **TensorBoard logging**.
- Analyzed training and validation metrics using TensorBoard.

### ğŸ§ª Model Details:
- **Loss Function:** Categorical Crossentropy
- **Optimizer:** Adam
- **Learning Rate:** 0.001
- **Epochs:** 5
- **Batch Size:** 32

### ğŸ“‚ TensorBoard Log Directory:
logs/fit/20250526-125927/train

yaml
Copy
Edit

### ğŸ“Š Observations:
- **Training Accuracy:** Improved to **0.9858**
- **Validation Accuracy:** Improved to **0.9773**
- **Training Loss:** Decreased to **0.0461**
- **Validation Loss:** Decreased to **0.074**
- Weight and bias histograms remained within expected distributions.

---

### ğŸ’¡ TensorBoard Questions Answered:

**1. What patterns do you observe in the accuracy curves?**  
Both training and validation accuracies increased smoothly and closely followed each other, indicating good generalization.

**2. How can you detect overfitting using TensorBoard?**  
Overfitting can be detected if training accuracy increases while validation accuracy plateaus or decreases. Similarly, a diverging validation loss is a strong indicator.

**3. What happens if the number of epochs increases?**  
The model may begin to overfit the training data unless regularization is applied. TensorBoard would show training loss decreasing while validation loss increases.

---

## ğŸ¥ Video Demonstration

ğŸ“¹ A 3-minute video demonstrating:
- The tensor operations
- Loss function comparisons and bar chart
- Live walkthrough of the TensorBoard dashboard

âœ… Submitted on BrightSpace along with this GitHub link.

---

## ğŸ’¬ Contact

If you have any questions, feel free to reach me at:  
âœ‰ï¸ [your-email@ucmo.edu]  
ğŸ“ GitHub Repository: [GitHub repo link here]
